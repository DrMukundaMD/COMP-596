{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import dataGenerator\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Company(object):\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.value = value\n",
    "\n",
    "with open('company_data.pkl', 'wb') as output:\n",
    "    company1 = Company('banana', 40)\n",
    "    pickle.dump(company1, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    company2 = Company('spam', 42)\n",
    "    pickle.dump(company2, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "del company1\n",
    "del company2\n",
    "\n",
    "with open('company_data.pkl', 'rb') as input:\n",
    "    company1 = pickle.load(input)\n",
    "    print(company1.name)  # -> banana\n",
    "    print(company1.value)  # -> 40\n",
    "\n",
    "    company2 = pickle.load(input)\n",
    "    print(company2.name) # -> spam\n",
    "    print(company2.value)  # -> 42\n",
    "\n",
    "    \n",
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# sample usage\n",
    "save_object(company1, 'company1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Values\n",
    "show_live_loss = False\n",
    "newTraining = False\n",
    "save_masks = True\n",
    "print_loss = True\n",
    "output_tensors = False\n",
    "train_flag = False\n",
    "EPOCHS = 5\n",
    "\n",
    "# default values\n",
    "NUM_CLASSES = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batchSize = 3\n",
    "iteration = \"1\"\n",
    "imageSize = 416\n",
    "cwd = os.getcwd()\n",
    "SAVE_LOCATION = cwd + \"/data/models/model_test\"\n",
    "LOAD_LOCATION = cwd + \"/data/models/model_test\"\n",
    "INTERRUPTED_LOCATION = cwd + \"/data/models/model_interrupted\"\n",
    "#data_dir = cwd + \"/data/photos/\"\n",
    "data_dir = \"./data/\"\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([transforms.Resize([imageSize,imageSize]),\n",
    "                                      transforms.ToTensor()\n",
    "                                     ])\n",
    "\n",
    "# instantiate the dataset and dataloader\n",
    "dataset = ImageFolderWithPaths(data_dir, transform=data_transforms) # our custom dataset\n",
    "\n",
    "#loads only photos\n",
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle=True)\n",
    "\n",
    "data_factory = dataGenerator.dataGenerator(IMAGE_SIZE=imageSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training W/O Val Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=4):\n",
    "    since = time.time()\n",
    "    best_model = None\n",
    "    best_loss = math.inf\n",
    "    #model.train()  \n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    sigmoid_function = nn.Sigmoid()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if print_loss:\n",
    "            print('Epoch {}/{}'.format(epoch + 1, num_epochs), flush=True)\n",
    "        epoch_loss = 0\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        logs = {}\n",
    "        \n",
    "        #BATCH TUPLE\n",
    "        inputs, labels, paths = next(iter(dataloaders))\n",
    "        inputs.to(device)\n",
    "        #labels.to(device)\n",
    "        \n",
    "        # Moved outside paths forloop as labels would be reset\n",
    "        i = 0\n",
    "        labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize).to(device)\n",
    "        \n",
    "        #build ground-truth batch tensor\n",
    "        for locations in paths:\n",
    "            #dtype=torch.int64\n",
    "            ### Why are we using float?\n",
    "            #labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize, dtype = torch.float32).to(device)\n",
    "            loc = locations.replace(\".png\", \".pt\").replace(\"photos\", \"tensors\")\n",
    "            labels[i] = torch.load(loc)\n",
    "            #print(\"loading tensor:\", i,  loc, flush=True)\n",
    "            #print(\"tensor max:\", torch.max(labels[i]))\n",
    "            i += 1\n",
    "            \n",
    "        if train_flag:\n",
    "            phase = 'train'\n",
    "            print(\"Setting model in training mode\")\n",
    "            # Set model to training mode\n",
    "            model.train() \n",
    "        else:\n",
    "            phase = 'val'\n",
    "            print(\"Setting model in validation mode\")\n",
    "            # Set model to evaluate mode\n",
    "            model.eval()\n",
    "               \n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            #build input-truth batch tensor\n",
    "            outputs = model(inputs.to(device)).to(device)\n",
    "            #outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels) #ground truth comparison\n",
    "            \n",
    "            if phase == 'train':\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # backward + optimize \n",
    "                loss.backward() #modifies model\n",
    "                optimizer.step() \n",
    "            \n",
    "                # statistics\n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "        outputs = sigmoid_function(outputs)\n",
    "        \n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         running_loss += loss.detach() * inputs.size(0)\n",
    "#         running_corrects += torch.sum(preds == labels.data, dtype = torch.float32)\n",
    "#         labels = labels.to(dtype=torch.long)\n",
    "#         running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "#         e_loss = running_loss / len(dataloaders.dataset)\n",
    "#         epoch_acc = running_corrects.double() / len(dataloaders.dataset)\n",
    "\n",
    "#         print(\"Running corrects: {}\".format(running_corrects), flush=True)\n",
    "        \n",
    "        \n",
    "        #logs['accuracy'] = epoch_acc\n",
    "        \n",
    "        \n",
    "        if show_live_loss and phase == 'train':\n",
    "            logs['log loss'] = epoch_loss\n",
    "            logs['accuracy'] = 1 - epoch_loss\n",
    "            liveloss.update(logs)\n",
    "            liveloss.draw()\n",
    "            \n",
    "        if save_masks and phase == 'val':\n",
    "            print(\"saving masks\")\n",
    "            outputs_dir = \"data/outputs/\" + str(datetime.now().date())\n",
    "            os.makedirs(outputs_dir, exist_ok=True)\n",
    "            threshold = 0.05\n",
    "            j = 0\n",
    "            for locations in paths:\n",
    "                img = Image.open(locations)\n",
    "                img = data_factory.showMaskOnImage(img,outputs[j], threshold)\n",
    "                img.save(outputs_dir  + \"/epoch_\" + str(epoch) + \"-\" + str(j) + \".png\",\"PNG\")\n",
    "                j += 1\n",
    "                \n",
    "        if print_loss:\n",
    "            print(\"loss: {}\".format(epoch_loss), flush=True)\n",
    "            print('---------------', flush=True)\n",
    "        \n",
    "        if output_tensors:\n",
    "            return inputs, outputs, paths\n",
    "        \n",
    "        #save best copy of  model\n",
    "        if phase == 'val' and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model, SAVE_LOCATION.replace(\"test\", \"best\"))\n",
    "            torch.save(model.state_dict(), SAVE_LOCATION.replace(\"test\", \"best\") + \"-\" + str(datetime.now().date()))\n",
    "            #torch.save(model, SAVE_LOCATION)\n",
    "            #torch.save(model, SAVE_LOCATION.replace(\"model\", \"model_best\"))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    \n",
    "    print('Training complete in {:.1f}m {:.1f}s'.format(time_elapsed // 60, time_elapsed % 60), flush=True)\n",
    "\n",
    "    #completed model\n",
    "    torch.save(model,SAVE_LOCATION)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pretrained Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from /home/royce/workspace/research/COMP-596/data/models/model_test\n"
     ]
    }
   ],
   "source": [
    "from unet_models import *\n",
    "\n",
    "#imports related to UNet\n",
    "if newTraining:\n",
    "    model = UNet16(num_classes=1, num_filters=32, pretrained=True, is_deconv=True)\n",
    "    \n",
    "    print('initializing model with random weights', flush=True)\n",
    "    torch.nn.init.xavier_uniform_(next(model.center.children())[1].weight)\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(next(model.dec5.children())[1].weight)\n",
    "    \n",
    "    torch.nn.init.xavier_uniform_(next(model.dec4.children())[1].weight)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(next(model.dec3.children())[1].weight)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(next(model.dec2.children())[1].weight)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(next(model.dec1.children()).weight)\n",
    "\n",
    "    torch.nn.init.xavier_uniform_(model.final.weight)\n",
    "                           \n",
    "else:\n",
    "    print(\"loading weights from\", LOAD_LOCATION, flush=True)\n",
    "    #model = torch.load(SAVE_LOCATION)\n",
    "    model = torch.load(LOAD_LOCATION)\n",
    "    #model.load_state_dict(best_model_wts)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "#criterion = DICELossMultiClass()\n",
    "#criterion = IOU_BCELoss()\n",
    "\n",
    "#Observe adjustments in learning rate\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=0.05,weight_decay=0, amsgrad=False, eps=0.1)\n",
    "\n",
    "# Osscilate between high and low learning rates over time\n",
    "exp_lr_scheduler = lr_scheduler.CosineAnnealingLR(optimizer_ft, T_max=EPOCHS,eta_min=0.001)  \n",
    "\n",
    "\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Setting model in validation mode\n",
      "saving masks\n",
      "loss: 0\n",
      "---------------\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/outputs/2019-03-22/batch_3-2.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2250bbab11dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINTERRUPTED_LOCATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9467934fde5b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;31m#labels = torch.zeros(batchSize,NUM_CLASSES,imageSize,imageSize, dtype = torch.float32).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"photos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tensors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;31m#print(\"loading tensor:\", i,  loc, flush=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m#print(\"tensor max:\", torch.max(labels[i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/outputs/2019-03-22/batch_3-2.pt'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=EPOCHS)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, INTERRUPTED_LOCATION)\n",
    "    print('Saved interrupt', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    inputs, outputs, paths = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=EPOCHS)\n",
    "    #batch = \n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model, INTERRUPTED_LOCATION)\n",
    "    print('Saved interrupt', flush=True)\n",
    "\n",
    "# for tensor in labels:\n",
    "#     #tensor = labels[0]\n",
    "#     tensor_max_value = torch.max(tensor)\n",
    "#     print(tensor_max_value)\n",
    "#     print(type(tensor_max_value.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dataGenerator.dataGenerator(416)\n",
    "j = 0\n",
    "outputs_dir = \"data/outputs/\" + str(datetime.now().date())\n",
    "os.makedirs(outputs_dir, exist_ok=True)\n",
    "threshold = 0.05\n",
    "batch = 3\n",
    "\n",
    "for locations in paths:\n",
    "    img = Image.open(locations)\n",
    "    img = dat.showMaskOnImage(img,outputs[j], threshold)\n",
    "    img.save(outputs_dir  + \"/batch_\" + str(batch) + \"-\" + str(j) + \".png\",\"PNG\")\n",
    "    #img.show()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
